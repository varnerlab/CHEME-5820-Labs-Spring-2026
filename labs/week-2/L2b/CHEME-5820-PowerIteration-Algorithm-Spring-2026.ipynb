{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e65d58",
   "metadata": {},
   "source": [
    "# Algorithm: Power Iteration Method\n",
    "The power iteration method can be used to compute the dominant eigenvalue and eigenvector of a matrix. Let's explore a pseudo-code implementation of the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration).\n",
    "\n",
    "__Initialization__: Given a real-valued square matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$, we specify $\\epsilon>0$ as the convergence tolerance, `maxiter` as the maximum number of iterations, an initial guess for the eigenvector $\\mathbf{v}_{1}^{(0)}$ (we'll use a random vector), an iteration counter $k\\gets{1}$, and a boolean variable $\\texttt{converged}\\gets{\\texttt{false}}$ to indicate whether the algorithm has converged.\n",
    "\n",
    "> __Parameter selection rules of thumb__:\n",
    "> \n",
    "> * __Convergence tolerance__ $\\epsilon$: A good starting point is $\\epsilon = 10^{-6}$ to $10^{-8}$ for most practical applications. For higher precision requirements, use $\\epsilon = 10^{-10}$ to $10^{-12}$. The choice depends on the desired accuracy and the condition number of the matrix.\n",
    "> \n",
    "> * __Maximum iterations__ `maxiter`: The number of iterations needed depends primarily on the eigenvalue separation ratio $r = |\\lambda_{2}|/|\\lambda_{1}|$, not on matrix size. The error decreases as $O(r^k)$, so the required iterations scale approximately as $k \\approx -\\log(\\epsilon)/\\log(1/r)$. For well-separated eigenvalues ($r \\leq 0.5$), use `maxiter = 100` to `500`. For moderate separation ($0.5 < r < 0.9$), use `maxiter = 500` to `2000`. For poorly separated eigenvalues ($r \\geq 0.9$), convergence may be very slow and require `maxiter â‰¥ 5000` or alternative methods. Matrix size affects computational time per iteration but not the number of iterations needed.\n",
    "> \n",
    "> * __When to worry about convergence__: If the algorithm doesn't converge within the maximum iterations, this typically indicates that the dominant eigenvalue is not well-separated from the second-largest eigenvalue ($|\\lambda_{1}|/|\\lambda_{2}| \\approx 1$), or the matrix may have complex eigenvalues with equal moduli. In such cases, consider using more sophisticated methods like the QR algorithm or Arnoldi iteration.\n",
    "\n",
    "Normalize the initial guess for the eigenvector: $\\mathbf{v}_{1}^{(0)} \\gets \\frac{\\mathbf{v}_{1}^{(0)}}{\\lVert \\mathbf{v}_{1}^{(0)} \\rVert_{2}}$.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "- Compute the matrix-vector product $\\mathbf{y}^{(k)}\\gets\\mathbf{A}\\mathbf{v}_{1}^{(k-1)}$.\n",
    "- Normalize the vector $\\mathbf{v}_{1}^{(k)}\\gets{\\mathbf{y}^{(k)}}/{\\lVert \\mathbf{y}^{(k)} \\rVert_{2}}$.\n",
    "- Compute the Rayleigh quotient to estimate the eigenvalue: $\\hat{\\lambda}_{1}^{(k)} \\gets  {\\mathbf{v}_{1}^{(k)\\top}\\mathbf{A}\\mathbf{v}_{1}^{(k)}}/{\\mathbf{v}_{1}^{(k)\\top}\\mathbf{v}_{1}^{(k)}}$.\n",
    "   1. If $\\lVert \\mathbf{v}_{1}^{(k)} - \\mathbf{v}_{1}^{(k-1)} \\rVert_{2}\\leq\\epsilon$, then set $\\texttt{converged}\\gets{\\texttt{true}}$ and return $\\hat{\\lambda}_{1} = \\hat{\\lambda}_{1}^{(k)}$ and $\\hat{\\mathbf{v}}_{1} = \\mathbf{v}_{1}^{(k)}$.\n",
    "   2. If $k\\geq\\texttt{maxiter}$, then set $\\texttt{converged}\\gets{\\texttt{true}}$ to terminate the algorithm, return the last estimate of the eigenvalue and eigenvector (optionally flagging that convergence tolerance was not met).\n",
    "   3. Increment $k\\gets{k+1}$.\n",
    "\n",
    "\n",
    "While simple to implement, the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) may exhibit slow convergence, mainly when the largest eigenvalue is close in magnitude to other eigenvalues, i.e., $|\\lambda_{1}|/|\\lambda_{2}| \\sim 1$.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
